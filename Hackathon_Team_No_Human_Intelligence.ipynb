{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Automated Settlement Valuation"
      ],
      "metadata": {
        "id": "xxTfGPQ6XIx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Can we build an LLM-powered Settlement Value Calculator for contested insurance claims? The goal is to produce defensible, probability weighted value ranges across litigation timelines. The tool would ingest policy terms, jurisdictional data, judge behavior, counsel behavior, motion practice, discovery, and insurer specific tactics. It must update dynamically, reflect procedural inflection points, spend, and support real-world capital decisions. These include choices around litigation spend, monetization, and settlement terms."
      ],
      "metadata": {
        "id": "6GLLFwAMWezt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQOSrlJ9w6eG",
        "outputId": "f68cc56b-ba32-4d8c-f78e-44716e47428e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install faiss-gpu"
      ],
      "metadata": {
        "id": "-7vFpNOxuQBY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv_TVwDy9xhT",
        "outputId": "dd02e125-c9ca-4706-b799-e4f56d23f47d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (37.4.0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import csv\n",
        "import random\n",
        "import datetime\n",
        "from typing import Dict, Any, List, Tuple\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from faker import Faker\n",
        "from transformers import (\n",
        "    pipeline as hf_pipeline,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer\n",
        ")\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.exceptions import NotFittedError\n",
        "from lightgbm import LGBMClassifier\n",
        "import joblib\n",
        "\n",
        "# Suppress transformers progress bars\n",
        "os.environ[\"TRANSFORMERS_NO_TQDM\"] = \"1\"\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "PIPELINE_KWARGS = {\"device\": 0} if USE_CUDA else {}\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "\n",
        "# Utility: build payoff matrix from discrete outcomes\n",
        "def build_payoff_matrix(outcomes: List[float], probs: List[float]) -> Dict[str, float]:\n",
        "    arr_o = np.array(outcomes, dtype=float)\n",
        "    arr_p = np.array(probs,    dtype=float)\n",
        "    mean  = float((arr_o * arr_p).sum())\n",
        "    var   = float(((arr_o - mean)**2 * arr_p).sum())\n",
        "    return {\"mean\": mean, \"std\": float(np.sqrt(var))}\n",
        "\n",
        "\n",
        "# DPR‐FAISS INDEXER STUB\n",
        "class DPRFaissIndexer:\n",
        "    def __init__(self, dim: int = 768):\n",
        "        self.index     = faiss.IndexFlatIP(dim)\n",
        "        self.doc_ids   = []\n",
        "        self.sentences = []\n",
        "        self.labels    = []\n",
        "\n",
        "    def add_sentences(self, annotations: List[Dict[str, Any]], doc_id: str):\n",
        "        for ann in annotations:\n",
        "            self.doc_ids.append(doc_id)\n",
        "            self.sentences.append(ann[\"sentence\"])\n",
        "            self.labels.append(ann.get(\"labels\", []))\n",
        "\n",
        "\n",
        "# Synthetic Data Generation\n",
        "class SyntheticDataGenerator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        jurisdictions: List[str],\n",
        "        judges:        List[str],\n",
        "        counsels:      List[str],\n",
        "        policy_types:  List[str]\n",
        "    ):\n",
        "        self.jurisdictions = jurisdictions\n",
        "        self.judges        = judges\n",
        "        self.counsels      = counsels\n",
        "        self.policy_types  = policy_types\n",
        "\n",
        "    def generate_case_metadata(self, case_id: str) -> Dict[str, Any]:\n",
        "        jurisdiction = random.choice(self.jurisdictions)\n",
        "        judge        = random.choice(self.judges)\n",
        "        counsel      = random.choice(self.counsels)\n",
        "        case_type    = random.choice(self.policy_types)\n",
        "        outcome      = random.choice([\"win\", \"loss\"])\n",
        "        filed_date   = fake.date_between(start_date=\"-3y\", end_date=\"today\").isoformat()\n",
        "        return {\n",
        "            \"case_id\":      case_id,\n",
        "            \"jurisdiction\": jurisdiction,\n",
        "            \"judge\":        judge,\n",
        "            \"counsel\":      counsel,\n",
        "            \"case_type\":    case_type,\n",
        "            \"case_outcome\": outcome,\n",
        "            \"filed_date\":   filed_date\n",
        "        }\n",
        "\n",
        "    def generate_spend_ledger(\n",
        "        self,\n",
        "        case_id:    str,\n",
        "        n_entries:  int = 5\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        ledger = []\n",
        "        start = datetime.datetime.strptime(\n",
        "            fake.date_between(start_date=\"-2y\", end_date=\"-1y\").isoformat(),\n",
        "            \"%Y-%m-%d\"\n",
        "        )\n",
        "        for i in range(n_entries):\n",
        "            spend_date = (start + datetime.timedelta(days=30 * i)).date().isoformat()\n",
        "            amount     = round(random.uniform(500, 20000), 2)\n",
        "            ledger.append({\n",
        "                \"case_id\":      case_id,\n",
        "                \"spend_date\":   spend_date,\n",
        "                \"spend_amount\": amount,\n",
        "                \"activity\":     random.choice([\"motion\", \"discovery\", \"expert\", \"mediation\"])\n",
        "            })\n",
        "        return ledger\n",
        "\n",
        "\n",
        "# PolicyParser\n",
        "class PolicyParser:\n",
        "\n",
        "    # Match dollar amounts with optional K/M/B suffix and per-claim/occurrence/year qualifiers\n",
        "    limit_pattern = re.compile(\n",
        "        r'(\\$\\s?[0-9\\.,]+\\s?(?:K|M|B)?(?:\\s?per[-\\s]?(?:claim|occurrence|year))?)',\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "    # Capture exclusions sections\n",
        "    exclusion_pattern = re.compile(\n",
        "        r'(?:Exclusion[s]?[:\\-]\\s*)(.+?)(?=\\n\\n|\\Z)',\n",
        "        re.IGNORECASE | re.DOTALL\n",
        "    )\n",
        "    # Capture endorsements sections\n",
        "    endorsement_pattern = re.compile(\n",
        "        r'(?:Endorsement[s]?[:\\-]\\s*)(.+?)(?=\\n\\n|\\Z)',\n",
        "        re.IGNORECASE | re.DOTALL\n",
        "    )\n",
        "\n",
        "    @staticmethod\n",
        "    def parse(text: str) -> Dict[str, Any]:\n",
        "        limits = PolicyParser.extract_limits(text)\n",
        "        exclusions = PolicyParser.extract_exclusions(text)\n",
        "        endorsements = PolicyParser.extract_endorsements(text)\n",
        "        return {\n",
        "            \"limits\": limits,\n",
        "            \"exclusions\": exclusions,\n",
        "            \"endorsements\": endorsements\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_limits(text: str) -> Dict[str, float]:\n",
        "        matches = PolicyParser.limit_pattern.findall(text)\n",
        "        parsed = {}\n",
        "        for m in matches:\n",
        "            cleaned = m.replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
        "            multiplier = 1\n",
        "            suffix = cleaned[-1].upper()\n",
        "            if suffix == \"K\":\n",
        "                multiplier = 1e3\n",
        "                cleaned = cleaned[:-1]\n",
        "            elif suffix == \"M\":\n",
        "                multiplier = 1e6\n",
        "                cleaned = cleaned[:-1]\n",
        "            try:\n",
        "                value = float(cleaned) * multiplier\n",
        "            except ValueError:\n",
        "                continue\n",
        "            key = \"limit\"\n",
        "            low = m.lower()\n",
        "            if \"per claim\" in low or \"per-claim\" in low:\n",
        "                key = \"per_claim\"\n",
        "            elif \"aggregate\" in low:\n",
        "                key = \"aggregate\"\n",
        "            else:\n",
        "                key = f\"limit_{len(parsed)+1}\"\n",
        "            parsed[key] = value\n",
        "        return parsed\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_exclusions(text: str) -> List[str]:\n",
        "        raw = PolicyParser.exclusion_pattern.findall(text)\n",
        "        return [r.strip() for r in raw]\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_endorsements(text: str) -> List[str]:\n",
        "        raw = PolicyParser.endorsement_pattern.findall(text)\n",
        "        return [r.strip() for r in raw]\n",
        "\n",
        "\n",
        "# Raw Data Collection\n",
        "class RawDataCollector:\n",
        "    def __init__(\n",
        "        self,\n",
        "        ocr_tool:    Any = None,\n",
        "        api_clients: Dict[str, Any] = None,\n",
        "        synth_gen:   SyntheticDataGenerator = None\n",
        "    ):\n",
        "        self.ocr_tool    = ocr_tool\n",
        "        self.api_clients = api_clients or {}\n",
        "        self.synth       = synth_gen\n",
        "\n",
        "    def ingest_pdf(self, pdf_path: str) -> Dict[str, Any]:\n",
        "        text = self.ocr_tool.extract_text(pdf_path) if self.ocr_tool else \"\"\n",
        "        parsed = PolicyParser.parse(text)\n",
        "        return {\n",
        "            \"type\":   \"pdf\",\n",
        "            \"text\":   text,\n",
        "            \"parsed\": parsed,\n",
        "            \"source\": pdf_path\n",
        "        }\n",
        "\n",
        "    def ingest_docket(self, docket_json: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        return {\"type\": \"docket\", \"data\": docket_json}\n",
        "\n",
        "    def ingest_spend_ledger(self, ledger_csv: str) -> List[Dict[str, Any]]:\n",
        "        records = []\n",
        "        try:\n",
        "            with open(ledger_csv) as f:\n",
        "                for row in csv.DictReader(f):\n",
        "                    records.append(row)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: spend ledger file not found: {ledger_csv}. Skipping.\")\n",
        "        return records\n",
        "\n",
        "    def ingest_manual(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        return data\n",
        "\n",
        "    def ingest_web_db(\n",
        "        self,\n",
        "        client_name: str,\n",
        "        endpoint:    str,\n",
        "        params:      Dict[str, Any] = None,\n",
        "        mode:        str = \"json\"\n",
        "    ) -> Dict[str, Any]:\n",
        "        client = self.api_clients.get(client_name)\n",
        "        if client is None:\n",
        "            raise ValueError(f\"No API client named {client_name}\")\n",
        "        if mode == \"json\":\n",
        "            data = client.fetch_json(endpoint, params)\n",
        "        elif mode == \"html\":\n",
        "            data = client.fetch_html(endpoint, params)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported mode: {mode}\")\n",
        "        return {\"type\": \"web\", \"source\": endpoint, \"data\": data}\n",
        "\n",
        "    def ingest_synthetic(self, case_id: str, n_spend: int = 5) -> List[Dict[str, Any]]:\n",
        "        if not self.synth:\n",
        "            raise RuntimeError(\"No SyntheticDataGenerator provided\")\n",
        "        meta   = self.synth.generate_case_metadata(case_id)\n",
        "        ledger = self.synth.generate_spend_ledger(case_id, n_entries=n_spend)\n",
        "        return [meta] + ledger\n",
        "\n",
        "\n",
        "# Web Scraper\n",
        "from typing import Dict, Any, Optional\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "class WebScraper:\n",
        "    def __init__(self, base_url: str = \"\", headers: Dict[str, str] = None,\n",
        "                 auth: Any = None, timeout: int = 10):\n",
        "        self.base_url = base_url\n",
        "        self.session  = requests.Session()\n",
        "        if headers:  self.session.headers.update(headers)\n",
        "        if auth:     self.session.auth = auth\n",
        "        self.timeout = timeout\n",
        "\n",
        "    def fetch_json(self, endpoint: str, params: Dict[str, Any] = None) -> Any:\n",
        "        url  = self.base_url + endpoint\n",
        "        resp = self.session.get(url, params=params, timeout=self.timeout)\n",
        "        resp.raise_for_status()\n",
        "        return resp.json()\n",
        "\n",
        "    def fetch_html(self, endpoint: str, params: Dict[str, Any] = None) -> BeautifulSoup:\n",
        "        url  = self.base_url + endpoint\n",
        "        resp = self.session.get(url, params=params, timeout=self.timeout)\n",
        "        resp.raise_for_status()\n",
        "        return BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    def extract_table(self, soup: BeautifulSoup, selector: str) -> pd.DataFrame:\n",
        "        table = soup.select_one(selector)\n",
        "        if table is None:\n",
        "            raise ValueError(f\"No table found for selector {selector}\")\n",
        "        return pd.read_html(str(table))[0]\n",
        "\n",
        "    def scrape_text(self, endpoint: str, selector: Optional[str] = None, params: Dict[str, Any] = None) -> str:\n",
        "        \"\"\"\n",
        "        Scrape all visible text from the specified endpoint.\n",
        "        Optionally, restrict to a CSS selector.\n",
        "        \"\"\"\n",
        "        soup = self.fetch_html(endpoint, params)\n",
        "        # Use selector if provided\n",
        "        if selector:\n",
        "            elements = soup.select(selector)\n",
        "            texts = [el.get_text(separator=\" \", strip=True) for el in elements]\n",
        "            return \"\\n\".join(texts)\n",
        "        else:\n",
        "            # Remove script/style elements\n",
        "            for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
        "                tag.extract()\n",
        "            text = soup.get_text(separator=\" \", strip=True)\n",
        "            return text\n",
        "\n",
        "# Text Annotation & Retrieval\n",
        "class SentenceAnnotator:\n",
        "    def __init__(self, categories: List[str],\n",
        "                 model_name: str = \"facebook/bart-large-mnli\"):\n",
        "        self.categories    = categories\n",
        "        self.model_name    = model_name\n",
        "        self._pipe         = None\n",
        "        self.split_pattern = re.compile(r'(?<=[\\.\\!\\?])\\s+')\n",
        "\n",
        "    @property\n",
        "    def pipe(self):\n",
        "        if self._pipe is None:\n",
        "            self._pipe = hf_pipeline(\"zero-shot-classification\",\n",
        "                                     model=self.model_name,\n",
        "                                     **PIPELINE_KWARGS)\n",
        "        return self._pipe\n",
        "\n",
        "    def annotate(self, text: str) -> List[Dict[str, Any]]:\n",
        "        sentences = [s.strip() for s in self.split_pattern.split(text) if s.strip()]\n",
        "        anns      = []\n",
        "        for sent in sentences:\n",
        "            out    = self.pipe(sent, self.categories, multi_label=True)\n",
        "            labels = [lbl for lbl, score in zip(out[\"labels\"], out[\"scores\"]) if score > 0.5]\n",
        "            anns.append({\"sentence\": sent, \"labels\": labels})\n",
        "        return anns\n",
        "\n",
        "# Feature Engineering\n",
        "class FeatureEngineer:\n",
        "    def __init__(self):\n",
        "        self.cat_encoder   = OneHotEncoder(sparse_output=False,\n",
        "                                           handle_unknown=\"ignore\")\n",
        "        self.judge_stats   = {}\n",
        "        self.counsel_stats = {}\n",
        "        self.fitted        = False\n",
        "\n",
        "    def fit(self, records: List[Dict[str, Any]]):\n",
        "        df = pd.DataFrame(records)\n",
        "        required = {'jurisdiction','case_type','judge','counsel','case_outcome'}\n",
        "        if not required.issubset(df.columns):\n",
        "            self.fitted = True\n",
        "            return\n",
        "\n",
        "        df = df.dropna(subset=['jurisdiction','case_type','case_outcome'])\n",
        "        self.cat_encoder.fit(df[['jurisdiction','case_type']].fillna(''))\n",
        "        df['is_win'] = (df['case_outcome']==\"win\").astype(int)\n",
        "        self.judge_stats   = df.groupby('judge')['is_win'].agg(['count','mean']).to_dict('index')\n",
        "        self.counsel_stats = df.groupby('counsel')['is_win'].agg(['count','mean']).to_dict('index')\n",
        "        self.fitted = True\n",
        "\n",
        "    def extract_features(self, records: List[Dict[str, Any]]) -> Dict[str, Dict[str, float]]:\n",
        "        if not self.fitted:\n",
        "            raise RuntimeError(\"FeatureEngineer must be fit() first.\")\n",
        "\n",
        "        df = pd.DataFrame(records)\n",
        "        if 'case_id' not in df.columns:\n",
        "            df['case_id'] = [r.get('case_id') for r in records]\n",
        "\n",
        "        required = {'jurisdiction','case_type','judge','counsel','case_id'}\n",
        "        if not required.issubset(df.columns):\n",
        "            return {}\n",
        "\n",
        "        df[\"filed_date\"]   = pd.to_datetime(df.get(\"filed_date\", pd.NaT))\n",
        "        df[\"spend_date\"]   = pd.to_datetime(df.get(\"spend_date\", pd.NaT))\n",
        "        df[\"spend_amount\"] = pd.to_numeric(df.get(\"spend_amount\", 0),\n",
        "                                           errors=\"coerce\").fillna(0)\n",
        "        df[\"days_since_filing\"] = (\n",
        "            pd.Timestamp.today() - df[\"filed_date\"] ).dt.days.fillna(-1)\n",
        "\n",
        "        cats      = df[['jurisdiction','case_type']].fillna('')\n",
        "        feats_ohe = self.cat_encoder.transform(cats)\n",
        "        cols_ohe  = self.cat_encoder.get_feature_names_out(\n",
        "                        ['jurisdiction','case_type'])\n",
        "        df_ohe    = pd.DataFrame(feats_ohe,\n",
        "                                 columns=cols_ohe,\n",
        "                                 index=df.index)\n",
        "\n",
        "        spend_sum = df.groupby('case_id')['spend_amount'].agg(['sum','mean','count'])\n",
        "\n",
        "        ts = (\n",
        "            df.set_index('spend_date')\n",
        "              .groupby('case_id')['spend_amount']\n",
        "              .resample('M').sum()\n",
        "              .unstack(fill_value=0)\n",
        "        )\n",
        "        ts.columns = [f\"spend_{d:%Y_%m}\" for d in ts.columns]\n",
        "\n",
        "        def lookup(stats,key,fld):\n",
        "            return stats.get(key,{}).get(fld,np.nan)\n",
        "\n",
        "        meta = df.drop_duplicates('case_id').set_index('case_id')\n",
        "        feats = pd.concat([\n",
        "            meta.apply(lambda r: lookup(self.judge_stats,   r[\"judge\"],   \"mean\"), axis=1).rename(\"judge_win_rate\"),\n",
        "            meta.apply(lambda r: lookup(self.counsel_stats, r[\"counsel\"], \"mean\"), axis=1).rename(\"counsel_win_rate\"),\n",
        "            spend_sum, ts, df_ohe\n",
        "        ],axis=1)\n",
        "\n",
        "        return feats.to_dict(orient='index')\n",
        "\n",
        "# Outcome Prediction with Calibrated LightGBM\n",
        "class OutcomePredictor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str = \"models/settlement_predictor_calibrated.pkl\",\n",
        "        buckets:    List[float] = None,\n",
        "        method:     str = \"sigmoid\",\n",
        "        cv:         int = 5\n",
        "    ):\n",
        "        self.model_path = model_path\n",
        "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
        "        try:\n",
        "            self.cal = joblib.load(model_path)\n",
        "        except FileNotFoundError:\n",
        "            base = LGBMClassifier(\n",
        "                objective='multiclass',\n",
        "                n_estimators=200,\n",
        "                learning_rate=0.05,\n",
        "                max_depth=6\n",
        "            )\n",
        "            self.cal = CalibratedClassifierCV(base, method=method, cv=cv)\n",
        "            if buckets is not None:\n",
        "                self.cal.classes_ = buckets\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        features: Dict[str, Dict[str, float]],\n",
        "        labels:   List[float]\n",
        "    ) -> None:\n",
        "        df_X = pd.DataFrame.from_dict(features, orient='index').fillna(0)\n",
        "        y    = pd.Series(labels, index=df_X.index).astype(float)\n",
        "        self.cal.fit(df_X, y)\n",
        "        joblib.dump(self.cal, self.model_path)\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        features: Dict[str, Dict[str, float]]\n",
        "    ) -> Tuple[List[float], List[float]]:\n",
        "        df_X = pd.DataFrame.from_dict(features, orient='index').fillna(0)\n",
        "        try:\n",
        "            proba = self.cal.predict_proba(df_X)[0]\n",
        "        except (AttributeError, NotFittedError):\n",
        "            base = getattr(self.cal, \"base_estimator\",\n",
        "                           getattr(self.cal, \"estimator\", None))\n",
        "            try:\n",
        "                proba = base.predict_proba(df_X)[0]\n",
        "            except:\n",
        "                proba = None\n",
        "\n",
        "        raw_classes = self.cal.classes_\n",
        "        buckets = [float(c) for c in raw_classes]\n",
        "        if proba is None:\n",
        "            proba = [1.0 / len(buckets)] * len(buckets)\n",
        "\n",
        "        return buckets, proba\n",
        "\n",
        "# Simulation & LLM Calibration\n",
        "class Simulator:\n",
        "    def simulate(self, payoff_matrix: Dict[str, float], n_simulations: int = 1000) -> List[float]:\n",
        "        μ = payoff_matrix.get(\"mean\", 0.0)\n",
        "        σ = payoff_matrix.get(\"std\",  1.0)\n",
        "        return np.random.normal(loc=μ, scale=σ, size=n_simulations).tolist()\n",
        "\n",
        "class Calibrator:\n",
        "    def __init__(self, llm_ckpt: str = \"t5-small\"):\n",
        "        self.llm_ckpt = llm_ckpt\n",
        "        self._pipe    = None\n",
        "\n",
        "    @property\n",
        "    def pipe(self):\n",
        "        if self._pipe is None:\n",
        "            model     = T5ForConditionalGeneration.from_pretrained(self.llm_ckpt)\n",
        "            tokenizer = T5Tokenizer.from_pretrained(self.llm_ckpt)\n",
        "            self._pipe = hf_pipeline(\n",
        "                \"text2text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                **PIPELINE_KWARGS\n",
        "            )\n",
        "        return self._pipe\n",
        "\n",
        "    def calibrate(self, sims: List[float], briefs: List[str]) -> str:\n",
        "        prompt = (\n",
        "            f\"Simulations:{sims}\\n\"\n",
        "            f\"Briefs:{'||'.join(briefs)}\\n\"\n",
        "            \"Produce adjusted probability weights and rationale.\"\n",
        "        )\n",
        "        out = self.pipe(prompt, max_length=256, truncation=True)\n",
        "        return out[0][\"generated_text\"].strip()\n",
        "\n",
        "# Litigation vs Settlement Risk Calculator\n",
        "class RiskCalculator:\n",
        "    def __init__(self, interest_rate: float):\n",
        "        self.r = interest_rate\n",
        "\n",
        "    def discount_factor(self, years: float) -> float:\n",
        "        return 1 / ((1 + self.r) ** years)\n",
        "\n",
        "    def build_discounted_payoff(self, outcomes: List[float], probs: List[float], time_horizon: float) -> Dict[str, float]:\n",
        "        d_f = self.discount_factor(time_horizon)\n",
        "        pv  = [o * d_f for o in outcomes]\n",
        "        arr_o = np.array(pv, dtype=float)\n",
        "        arr_p = np.array(probs, dtype=float)\n",
        "        mean  = float((arr_o * arr_p).sum())\n",
        "        var   = float(((arr_o - mean)**2 * arr_p).sum())\n",
        "        return {\"mean\": mean, \"std\": float(np.sqrt(var))}\n",
        "\n",
        "    def compare(\n",
        "        self,\n",
        "        settle_outcomes: List[float], settle_probs: List[float], settle_time: float,\n",
        "        litig_outcomes:  List[float], litig_probs:  List[float], litig_time:  float\n",
        "    ) -> Dict[str, Any]:\n",
        "        ps = self.build_discounted_payoff(settle_outcomes, settle_probs, settle_time)\n",
        "        pl = self.build_discounted_payoff(litig_outcomes,  litig_probs,  litig_time)\n",
        "\n",
        "        n  = 10000\n",
        "        ds = np.random.choice(settle_outcomes, size=n, p=settle_probs) * self.discount_factor(settle_time)\n",
        "        dl = np.random.choice(litig_outcomes,  size=n, p=litig_probs ) * self.discount_factor(litig_time)\n",
        "        diff = dl - ds\n",
        "\n",
        "        return {\n",
        "            \"PV_settle_mean\":       ps[\"mean\"],\n",
        "            \"PV_settle_std\":        ps[\"std\"],\n",
        "            \"PV_litig_mean\":        pl[\"mean\"],\n",
        "            \"PV_litig_std\":         pl[\"std\"],\n",
        "            \"mean_diff\":            float(diff.mean()),\n",
        "            \"std_diff\":             float(diff.std()),\n",
        "            \"prob_litig_gt_settle\": float((diff > 0).mean())\n",
        "        }\n",
        "\n",
        "# Live Monitor\n",
        "class LiveMonitor:\n",
        "    def __init__(self, ecf_client: Any = None, spend_client: Any = None):\n",
        "        self.ecf   = ecf_client\n",
        "        self.spend = spend_client\n",
        "\n",
        "    def fetch_updates(self) -> Dict[str, Any]:\n",
        "        return {}\n",
        "\n",
        "    def detect_anomalies(self, data: Dict[str, Any]) -> bool:\n",
        "        return False\n",
        "\n",
        "    def trigger_rerun(self, callback: Any):\n",
        "        data = self.fetch_updates()\n",
        "        if self.detect_anomalies(data):\n",
        "            callback()\n",
        "\n",
        "# Full Legal RAG Pipeline\n",
        "class LegalRAGPipeline:\n",
        "    def __init__(\n",
        "        self,\n",
        "        categories:          List[str],\n",
        "        sub_texts:           List[str],\n",
        "        jur_rules:           Dict[str, Any],\n",
        "        predictor_path:      str   = \"models/settlement_predictor_calibrated.pkl\",\n",
        "        llm_ckpt:            str   = \"t5-small\",\n",
        "        interest_rate:       float = 0.05,\n",
        "        default_settle_time: float = 1.0,\n",
        "        default_litig_time:  float = 1.0\n",
        "    ):\n",
        "        self.default_settle_time = default_settle_time\n",
        "        self.default_litig_time  = default_litig_time\n",
        "\n",
        "        self.jur_rules = jur_rules\n",
        "        self.synth     = SyntheticDataGenerator(\n",
        "            jurisdictions=list(jur_rules.keys()),\n",
        "            judges=[\"Judge A\",\"Judge B\",\"Judge C\"],\n",
        "            counsels=[\"Counsel X\",\"Counsel Y\",\"Counsel Z\"],\n",
        "            policy_types=[\"property\",\"liability\",\"subrogation\"]\n",
        "        )\n",
        "        self.raw        = RawDataCollector(\n",
        "            ocr_tool=None,\n",
        "            api_clients={},\n",
        "            synth_gen=self.synth\n",
        "        )\n",
        "        self.fe         = FeatureEngineer()\n",
        "        self._annotator = None\n",
        "        self._indexer   = None\n",
        "        self._predictor = None\n",
        "        self._sim       = Simulator()\n",
        "        self._cal       = None\n",
        "        self._risk      = RiskCalculator(interest_rate)\n",
        "        self._monitor   = LiveMonitor()\n",
        "\n",
        "        self.categories     = categories\n",
        "        self.sub_texts      = sub_texts\n",
        "        self.predictor_path = predictor_path\n",
        "        self.llm_ckpt       = llm_ckpt\n",
        "\n",
        "    @property\n",
        "    def annotator(self):\n",
        "        if self._annotator is None:\n",
        "            self._annotator = SentenceAnnotator(self.categories)\n",
        "        return self._annotator\n",
        "\n",
        "    @property\n",
        "    def indexer(self):\n",
        "        if self._indexer is None:\n",
        "            self._indexer = DPRFaissIndexer()\n",
        "            anns = [{\"sentence\": t, \"labels\": [\"subrogation\"]} for t in self.sub_texts]\n",
        "            self._indexer.add_sentences(anns, doc_id=\"subrog_templates\")\n",
        "        return self._indexer\n",
        "\n",
        "    @property\n",
        "    def predictor(self):\n",
        "        if self._predictor is None:\n",
        "            self._predictor = OutcomePredictor(\n",
        "                self.predictor_path,\n",
        "                buckets=[0, 50000, 100000, 150000]\n",
        "            )\n",
        "        return self._predictor\n",
        "\n",
        "    @property\n",
        "    def simulator(self):\n",
        "        return self._sim\n",
        "\n",
        "    @property\n",
        "    def calibrator(self):\n",
        "        if self._cal is None:\n",
        "            self._cal = Calibrator(self.llm_ckpt)\n",
        "        return self._cal\n",
        "\n",
        "    @property\n",
        "    def risk_calc(self):\n",
        "        return self._risk\n",
        "\n",
        "    @property\n",
        "    def monitor(self):\n",
        "        return self._monitor\n",
        "\n",
        "    def ingest_case(\n",
        "        self,\n",
        "        case_id:       str,\n",
        "        pdf_path:      str              = None,\n",
        "        docket_json:   Dict[str, Any]   = None,\n",
        "        ledger_csv:    str              = None,\n",
        "        manual_data:   Dict[str, Any]   = None,\n",
        "        use_synthetic: bool             = False,\n",
        "        synth_spend:   int              = 5\n",
        "    ) -> Dict[str, Any]:\n",
        "        raw, struct = [], []\n",
        "        if use_synthetic:\n",
        "            struct += self.raw.ingest_synthetic(case_id, n_spend=synth_spend)\n",
        "        else:\n",
        "            if pdf_path:\n",
        "                raw.append(self.raw.ingest_pdf(pdf_path))\n",
        "            if docket_json:\n",
        "                r = self.raw.ingest_docket(docket_json)\n",
        "                raw.append(r); struct.append(r[\"data\"])\n",
        "            if ledger_csv:\n",
        "                struct += self.raw.ingest_spend_ledger(ledger_csv)\n",
        "            if manual_data:\n",
        "                r = self.raw.ingest_manual(manual_data)\n",
        "                raw.append(r); struct.append(r)\n",
        "\n",
        "        if not self.fe.fitted or use_synthetic:\n",
        "            self.fe.fit(struct)\n",
        "\n",
        "        full_text = \"\".join(r.get(\"text\", \"\") for r in raw)\n",
        "        anns      = self.annotator.annotate(full_text)\n",
        "        self.indexer.add_sentences(anns, doc_id=case_id)\n",
        "\n",
        "        return self.fe.extract_features(struct)\n",
        "\n",
        "    def simulate_settlement(\n",
        "        self,\n",
        "        case_id:       str,\n",
        "        pdf_path:      str            = None,\n",
        "        docket_json:   Dict[str, Any] = None,\n",
        "        ledger_csv:    str            = None,\n",
        "        manual_data:   Dict[str, Any] = None,\n",
        "        briefs:        List[str]      = [],\n",
        "        n_draws:       int            = 10000,\n",
        "        use_synthetic: bool           = False,\n",
        "        synth_spend:   int            = 5\n",
        "    ) -> Dict[str, Any]:\n",
        "        feats           = self.ingest_case(\n",
        "            case_id, pdf_path, docket_json,\n",
        "            ledger_csv, manual_data,\n",
        "            use_synthetic, synth_spend\n",
        "        )\n",
        "        buckets, weights = self.predictor.predict(feats)\n",
        "        payoff          = build_payoff_matrix(buckets, weights)\n",
        "        sims            = self.simulator.simulate(payoff, n_simulations=n_draws)\n",
        "        cal             = self.calibrator.calibrate(sims, briefs)\n",
        "        return {\n",
        "            \"buckets\":       buckets,\n",
        "            \"weights\":       weights,\n",
        "            \"payoff_matrix\": payoff,\n",
        "            \"simulations\":   sims,\n",
        "            \"calibration\":   cal\n",
        "        }\n",
        "\n",
        "    def simulate_litigation_vs_settlement(\n",
        "        self,\n",
        "        settle_outcomes: List[float],\n",
        "        settle_probs:    List[float],\n",
        "        settle_time:     float             = None,\n",
        "        litig_outcomes:  List[float]       = None,\n",
        "        litig_probs:     List[float]       = None,\n",
        "        litig_time:      float             = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        if settle_time is None:\n",
        "            settle_time = self.default_settle_time\n",
        "        if litig_time is None:\n",
        "            litig_time = self.default_litig_time\n",
        "        if litig_outcomes is None or litig_probs is None:\n",
        "            litig_outcomes = settle_outcomes\n",
        "            litig_probs    = settle_probs\n",
        "\n",
        "        return self.risk_calc.compare(\n",
        "            settle_outcomes, settle_probs, settle_time,\n",
        "            litig_outcomes,  litig_probs,  litig_time\n",
        "        )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    briefs = [\"Plaintiff’s opening brief…\", \"Defendant’s MSJ…\"]\n",
        "\n",
        "    rag = LegalRAGPipeline(\n",
        "        categories           = [\"subrogation\", \"coverage\", \"liability\"],\n",
        "        sub_texts            = [\"If subrogation is available…\", \"Subrogation clause states…\"],\n",
        "        jur_rules            = {\"NY\": {\"statute_of_limitations\": 3}},\n",
        "        predictor_path       = \"models/settlement_predictor_calibrated.pkl\",\n",
        "        interest_rate        = 0.05,\n",
        "        default_settle_time  = 1.0,\n",
        "        default_litig_time   = 1.0\n",
        "    )\n",
        "\n",
        "    # 1) Settlement simulation + calibration\n",
        "    sim_result = rag.simulate_settlement(\n",
        "        case_id   = \"CASE123\",\n",
        "        pdf_path  = \"data/CASE123.pdf\",\n",
        "        briefs    = briefs,\n",
        "        n_draws   = 10000\n",
        "    )\n",
        "\n",
        "    # 2) Litigation vs Settlement risk comparison\n",
        "    risk_metrics = rag.simulate_litigation_vs_settlement(\n",
        "        settle_outcomes = sim_result[\"buckets\"],\n",
        "        settle_probs    = sim_result[\"weights\"]\n",
        "    )\n",
        "\n",
        "    # Build and print tables\n",
        "    # Bucket probabilities\n",
        "    df_buckets = pd.DataFrame({\n",
        "        \"bucket\": sim_result[\"buckets\"],\n",
        "        \"probability\": sim_result[\"weights\"]\n",
        "    })\n",
        "    print(\"\\n## Bucket Probabilities\\n\")\n",
        "    print(df_buckets.to_markdown(index=False))\n",
        "\n",
        "    # Summary metrics\n",
        "    summary = {\n",
        "        \"Settle Mean (undiscounted)\": sim_result[\"payoff_matrix\"][\"mean\"],\n",
        "        \"Settle Std  (undiscounted)\": sim_result[\"payoff_matrix\"][\"std\"],\n",
        "        \"PV Settle Mean\":              risk_metrics[\"PV_settle_mean\"],\n",
        "        \"PV Settle Std\":               risk_metrics[\"PV_settle_std\"],\n",
        "        \"PV Litig Mean\":               risk_metrics[\"PV_litig_mean\"],\n",
        "        \"PV Litig Std\":                risk_metrics[\"PV_litig_std\"],\n",
        "        \"Mean Diff (Litig - Settle)\":  risk_metrics[\"mean_diff\"],\n",
        "        \"Std Diff\":                    risk_metrics[\"std_diff\"],\n",
        "        \"P(Litig > Settle)\":           risk_metrics[\"prob_litig_gt_settle\"],\n",
        "        \"LLM Calibration\":             sim_result[\"calibration\"]\n",
        "    }\n",
        "    df_summary = pd.DataFrame.from_dict(summary, orient=\"index\", columns=[\"value\"])\n",
        "    print(\"\\n## Summary Metrics\\n\")\n",
        "    print(df_summary.to_markdown())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP3qxTDVUnPC",
        "outputId": "7fa62079-73ef-421a-b71f-a1b04ae32096"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## Bucket Probabilities\n",
            "\n",
            "|   bucket |   probability |\n",
            "|---------:|--------------:|\n",
            "|        0 |          0.25 |\n",
            "|    50000 |          0.25 |\n",
            "|   100000 |          0.25 |\n",
            "|   150000 |          0.25 |\n",
            "\n",
            "## Summary Metrics\n",
            "\n",
            "|                            | value                                                                                                                                                                                                                               |\n",
            "|:---------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| Settle Mean (undiscounted) | 75000.0                                                                                                                                                                                                                             |\n",
            "| Settle Std  (undiscounted) | 55901.69943749474                                                                                                                                                                                                                   |\n",
            "| PV Settle Mean             | 71428.57142857142                                                                                                                                                                                                                   |\n",
            "| PV Settle Std              | 53239.71374999498                                                                                                                                                                                                                   |\n",
            "| PV Litig Mean              | 71428.57142857142                                                                                                                                                                                                                   |\n",
            "| PV Litig Std               | 53239.71374999498                                                                                                                                                                                                                   |\n",
            "| Mean Diff (Litig - Settle) | -128.57142857142856                                                                                                                                                                                                                 |\n",
            "| Std Diff                   | 74979.85685662758                                                                                                                                                                                                                   |\n",
            "| P(Litig > Settle)          | 0.3709                                                                                                                                                                                                                              |\n",
            "| LLM Calibration            | , -18892.19208792975, -63915.095045583, 99119.7661407096, 173207.18949071347, 119229.52267031628, -62004.60969563047, -63915.095045583, 99119.7661407096, 173207.18949071347, 119229.52267031628, 126415.94652077509, 101959.680894 |\n"
          ]
        }
      ]
    }
  ]
}